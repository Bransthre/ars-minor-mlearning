\contentsline {part}{I\hspace {1em}Entering the Fundamentals of Machine Learning}{3}{part.1}%
\contentsline {chapter}{\numberline {1}DATA C100: Introduction to Modeling and Linear Regression}{4}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{4}{section.1.1}%
\contentsline {section}{\numberline {1.2}A Demonstration: Simple Linear Regression}{4}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}What is a Regression Line?}{4}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Model Selection: Simple Linear Regression}{6}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Quantifying Errors: Loss Functions}{6}{subsection.1.2.3}%
\contentsline {chapter}{\numberline {2}DATA C100: Constant Model in Linear Regression}{9}{chapter.2}%
\contentsline {section}{\numberline {2.1}Defining the Constant Model}{9}{section.2.1}%
\contentsline {section}{\numberline {2.2}Experimenting Different Loss Functions}{9}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Exploring L2 Loss: MSE}{9}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Exploring L1 Loss: MAE}{10}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Summary of Loss Function Optimization}{11}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Transformation and Model Linearity}{12}{section.2.3}%
\contentsline {chapter}{\numberline {3}DATA C100: Ordinary Least Squares}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Multiple Linear Regression Model}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Optimization of Model: Least Squares Algorithm}{14}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Loss Function}{14}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Optimization via Geometric Interpretation}{14}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Performance Factors}{16}{section.3.3}%
\contentsline {chapter}{\numberline {4}DATA C100: Gradient Descent}{17}{chapter.4}%
\contentsline {section}{\numberline {4.1}Computational Minimization of Loss Function}{17}{section.4.1}%
\contentsline {section}{\numberline {4.2}Gradient Descent Algorithm}{17}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Interpretation of Gradients}{18}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Improving the Gradient Descent Sketch}{19}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Convexity}{20}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4}Stochastic Gradient Descent}{21}{subsection.4.2.4}%
\contentsline {chapter}{\numberline {5}DATA C100: Feature Engineering}{22}{chapter.5}%
\contentsline {section}{\numberline {5.1}Motivation and Generalization}{22}{section.5.1}%
\contentsline {section}{\numberline {5.2}One Hot Encoding}{22}{section.5.2}%
\contentsline {chapter}{\numberline {6}DATA C100: Cross Validation}{24}{chapter.6}%
\contentsline {section}{\numberline {6.1}Variance and Training Error}{24}{section.6.1}%
\contentsline {section}{\numberline {6.2}Validation}{25}{section.6.2}%
\contentsline {section}{\numberline {6.3}K-Fold Cross Validation}{26}{section.6.3}%
\contentsline {section}{\numberline {6.4}Test Set}{27}{section.6.4}%
\contentsline {chapter}{\numberline {7}DATA C100: Regularization}{28}{chapter.7}%
\contentsline {section}{\numberline {7.1}General Introduction to Regularization}{28}{section.7.1}%
\contentsline {section}{\numberline {7.2}Choices of Regularization}{29}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}L2 Regularization (Ridge)}{29}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}L1 Regularization (LASSO)}{30}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Summary of Regularization Choices}{30}{subsection.7.2.3}%
